{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:04:09.782440Z","iopub.status.busy":"2023-12-02T12:04:09.782077Z","iopub.status.idle":"2023-12-02T12:04:49.876572Z","shell.execute_reply":"2023-12-02T12:04:49.875456Z","shell.execute_reply.started":"2023-12-02T12:04:09.782409Z"},"trusted":true},"outputs":[],"source":["!pip install -q seqeval\n","!pip install -q evaluate\n","!pip install -q pytorch-crf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:04:49.878804Z","iopub.status.busy":"2023-12-02T12:04:49.878495Z","iopub.status.idle":"2023-12-02T12:05:03.675202Z","shell.execute_reply":"2023-12-02T12:05:03.674379Z","shell.execute_reply.started":"2023-12-02T12:04:49.878778Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import evaluate\n","import pickle\n","import random\n","import tqdm\n","import re\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","import torch.nn as nn\n","from torchcrf import CRF\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n","from transformers import get_scheduler\n","from huggingface_hub import Repository, get_full_repo_name\n","from huggingface_hub import login\n","from accelerate import Accelerator\n","from tqdm.auto import tqdm\n","from datasets import *"]},{"cell_type":"markdown","metadata":{},"source":["## Create NER labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:05:03.676603Z","iopub.status.busy":"2023-12-02T12:05:03.676336Z","iopub.status.idle":"2023-12-02T12:05:03.685405Z","shell.execute_reply":"2023-12-02T12:05:03.683769Z","shell.execute_reply.started":"2023-12-02T12:05:03.676581Z"},"trusted":true},"outputs":[],"source":["entity = ['PATIENT'   , 'DOCTOR'       , 'USERNAME'  ,\n","          'PROFESSION',\n","          'ROOM'      , 'DEPARTMENT'   , 'HOSPITAL'  , 'ORGANIZATION', 'STREET' , 'CITY'    , 'STATE' , 'COUNTRY', 'ZIP'  , 'LOCATION-OTHER', \n","          'AGE'       , \n","          'DATE'      , 'TIME'         , 'DURATION'  , 'SET'         , \n","          'PHONE'     , 'FAX'          , 'EMAIL'     , 'URL'         , 'IPADDR' , \n","          'SSN'       , 'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT'     , 'LICENSE', 'VECHICLE', 'DEVICE', 'BIOID'  , 'IDNUM']\n","label_names = ['OTHER']\n","entity_names = []\n","entity_count = [0] * len(entity)\n","\n","for s in entity:\n","    label_names.append(f'B-{s}')\n","    label_names.append(f'I-{s}')\n","    entity_names.append(s)\n","    \n","id2label = {i: label for i, label in enumerate(label_names)}\n","label2id = {v: k for k, v in id2label.items()}\n","org_id2label = {i: label for i, label in enumerate(entity_names)}\n","org_label2id = {v: k for k, v in org_id2label.items()}"]},{"cell_type":"markdown","metadata":{},"source":["## Regular expression to find pattern need to be normalized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:05:03.688680Z","iopub.status.busy":"2023-12-02T12:05:03.688294Z","iopub.status.idle":"2023-12-02T12:05:03.699517Z","shell.execute_reply":"2023-12-02T12:05:03.698577Z","shell.execute_reply.started":"2023-12-02T12:05:03.688655Z"},"trusted":true},"outputs":[],"source":["DATEs = '(\\d{1,2}\\/\\d{2,5})|(\\/\\d{1,2}\\/(\\d{2}|\\d{4}))|(\\d{1,2}(\\/|\\.| |-|,)\\d{1,2}(\\/|\\.| |-|,)\\d{2,4})|(\\d{3})|(\\d{4})|(\\d{8})|((\\d{1,2}|)( |)(January|February|March|April|May|June|July|August|September|October|November|December) \\d{4})|(Today|today|Now|now|Original|original|Previous|previous)|(Sunday|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday)|((\\d{2}|)(-|)(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-| )\\d{2,4})|(\\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4})'\n","\n","TIMEs = '((\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}(  | |)|)(at|)( |)\\d{1,2}(:|\\.)\\d{2}(AM|am|PM|pm|Hr|Hrs|hr|hrs|)( on the \\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}|))|(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|((at |)(\\d{1,2}|)(:|\\.|)\\d{2}( |)(am|pm|Hr|Hrs|hr|hrs|)( on | )(the |)\\d{1,2}(\\/|\\.)\\d{2,4}(\\/|\\.)\\d{1,2})|(((\\d{1,2}((pm)|(am)))|(\\d{4}(Hr|Hrs|hr|hrs|)))(( on )| )\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4})'\n","\n","DURATIONs = '((\\d{1,2}|\\d{1,2}-\\d{1,2}|two|five)( |\\/|)(day|days|week|weeks|wk|wks|month|months|year|years|yr|yrs))'\n","\n","SETs = 'twice'"]},{"cell_type":"markdown","metadata":{},"source":["## Create dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:05:03.701319Z","iopub.status.busy":"2023-12-02T12:05:03.700789Z","iopub.status.idle":"2023-12-02T12:05:03.776667Z","shell.execute_reply":"2023-12-02T12:05:03.775533Z","shell.execute_reply.started":"2023-12-02T12:05:03.701293Z"},"trusted":true},"outputs":[],"source":["# use argment \"ans\" to compare normalized string and ground truth\n","\n","def Normalize(time_type, org, ans):\n","    nor = ''\n","    if (time_type == 'DATE'):\n","        if (re.match('\\d{1,2}(\\/|\\.| |-|,)\\d{1,2}(\\/|\\.| |-|,)\\d{2,4}', org)):\n","            l = re.split('\\/|\\.| |-|,', org)\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            elif (len(l[2]) == 3):\n","                l[2] = '2' + l[2]\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\/\\d{1,2}\\/(\\d{2}|\\d{4})', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            nor = l[2] + '-' + l[1]\n","        elif (re.match('\\d{1,2}\\/\\d{2,5}', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            if (len(l[1]) == 2):\n","                nor = '20' + l[1] + '-' + l[0]\n","            elif (len(l[1]) == 3):\n","                nor = '20' + l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","            elif (len(l[1]) == 4):\n","                nor = l[1] + '-' + l[0]\n","            elif (len(l[1]) == 5):\n","                nor = l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","        elif (re.match('\\d{8}', org)):\n","            nor = org[0:4] + '-' + org[4:6] + '-' + org[6:8]\n","        elif (re.match('\\d{4}', org)):\n","            nor = org\n","        elif (re.match('\\d{3}', org)):\n","            nor = '2' + org\n","        elif (re.match('(\\d{2}|)(-|)(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-| )\\d{2,4}', org)):\n","            org = org.replace('Jan', '01')\n","            org = org.replace('Feb', '02')\n","            org = org.replace('Mar', '03')\n","            org = org.replace('Apr', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('Jun', '06')\n","            org = org.replace('Jul', '07')\n","            org = org.replace('Aug', '08')\n","            org = org.replace('Sep', '09')\n","            org = org.replace('Oct', '10')\n","            org = org.replace('Nov', '11')\n","            org = org.replace('Dec', '12')\n","            l = re.split('-| ', org)\n","            if (len(l) == 2):\n","                if (len(l[1]) == 2):\n","                    l[1] = '20' + l[1]\n","                elif (len(l[1]) == 3):\n","                    l[1] = '2' + l[1]\n","                nor = l[1] + '-' + l[0]\n","            else:\n","                if (len(l[2]) == 2):\n","                    l[2] = '20' + l[2]\n","                elif (len(l[2]) == 3):\n","                    l[2] = '2' + l[2]\n","                nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            l = re.split(' ', org)\n","            nor = l[3] + '-' + l[2] + '-' + l[0][:-2]\n","        elif (re.match('(\\d{1,2}|)( |)(January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            if (re.match('\\d', org[0]) and re.match('\\d', org[1]) == None):\n","                org = '0' + org\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            org = org.replace(' ', '')\n","            if (len(org) == 6):\n","                nor = org[2:] + '-' + org[0:2]\n","            else:    \n","                nor = org[4:] + '-' + org[2:4] + '-' + org[0:2]\n","    elif (time_type == 'TIME'):\n","        if (re.match('(\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}(  | |)|)(at|)( |)\\d{1,2}(:|\\.)\\d{2}(AM|am|PM|pm|Hr|Hrs|hr|hrs|)( on the \\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}|)', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('PM', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('AM', org, flags=0) != None):\n","                am = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            get_date = 0\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0]\n","                get_date = 1\n","            yyyy = re.search('\\d{4}', org, flags=0)\n","            if (yyyy != None and get_date == 0):\n","                yyyy = yyyy.group(0)\n","                org = org.replace(yyyy, '')\n","                nor = yyyy + '-'\n","            mm = re.search('January|February|March|April|May|June|July|August|September|October|November|December', org, flags=0)\n","            if (mm != None and get_date == 0):\n","                mm = mm.group(0)\n","                org = org.replace(mm, '')\n","                mm = mm.replace('January', '01')\n","                mm = mm.replace('Feburary', '02')\n","                mm = mm.replace('March', '03')\n","                mm = mm.replace('April', '04')\n","                mm = mm.replace('May', '05')\n","                mm = mm.replace('June', '06')\n","                mm = mm.replace('July', '07')\n","                mm = mm.replace('August', '08')\n","                mm = mm.replace('September', '09')\n","                mm = mm.replace('October', '10')\n","                mm = mm.replace('November', '11')\n","                mm = mm.replace('December', '12')\n","                nor = nor + mm + '-'\n","            dd = re.search('\\d{1,2}((st)|(nd)|(rd)|(th))', org, flags=0)\n","            if (dd != None and get_date == 0):\n","                dd = dd.group(0)\n","                org = org.replace(dd, '')\n","                dd = dd.replace('st', '')\n","                dd = dd.replace('nd', '')\n","                dd = dd.replace('rd', '')\n","                dd = dd.replace('th', '')\n","                if (len(dd) == 1):\n","                    dd = '0' + dd\n","                nor = nor + dd\n","            get_time = 0\n","            time = re.search('\\d{1,2}(:|\\.)\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                time = re.split('\\.|:', time)\n","                if (pm == 1 and int(time[0]) < 12):\n","                    time[0] = str(int(time[0]) + 12)\n","                elif (am == 1 and int(time[0]) == 12):\n","                    time[0] = '00'\n","                if (len(time[0]) == 1):\n","                    time[0] = '0' + time[0]\n","                nor = nor + 'T' + time[0] + ':' + time[1]\n","                get_time = 1\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None and get_time == 0):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + 'T' + hh + ':' + mm    \n","            #if (nor != ans):    \n","                #print(f'1:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', org)):\n","            tmp = org\n","            nor = org.replace(' ', 'T')\n","            #if (nor != ans):    \n","                #print(f'2:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('(at |)(\\d{1,2}|)(:|\\.|)\\d{2}( |)(am|pm|Hr|Hrs|hr|hrs|)( on | )(the |)\\d{1,2}(\\/|\\.)\\d{2,4}(\\/|\\.)\\d{1,2}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            org = org.replace(':', '')\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + hh + ':' + mm\n","            #if (nor != ans):    \n","                #print(f'3:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('((\\d{1,2}((pm)|(am)))|(\\d{4}(Hr|Hrs|hr|hrs|)))(( on )| )\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            hrtime = re.search('\\d{4}', org, flags=0)\n","            if (hrtime != None):\n","                hrtime = hrtime.group(0)\n","                org = org.replace(hrtime, '')\n","                nor = nor + hrtime[0:2] + ':' + hrtime[2:]\n","            time = re.search('\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                if (len(hh) == 1):\n","                    hh = '0' + hh\n","                nor = nor + hh + ':' + '00'\n","            #if (nor != ans):    \n","                #print(f'4:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'DURATION'):\n","        tmp = org\n","        org = org.replace('one', '1')\n","        org = org.replace('two', '2')\n","        org = org.replace('three', '3')\n","        org = org.replace('four', '4')\n","        org = org.replace('five', '5')\n","        num = ''\n","        alp = ''\n","        for i in range(len(org)):\n","            if (org[i] == 'D' or org[i] == 'd' or\\\n","                org[i] == 'W' or org[i] == 'w' or\\\n","                org[i] == 'M' or org[i] == 'm' or\\\n","                org[i] == 'Y' or org[i] == 'y'):\n","                alp = org[i]\n","                org = org[:i]\n","                break\n","        org = re.split('-| ', org)\n","        if (len(org) == 1 or org[1] == ''):\n","            nor = 'P' + org[0] + alp.upper()\n","        else:\n","            nor = 'P' + str((int(org[0]) + int(org[1])) / 2) + alp.upper()\n","        if (nor != ans):    \n","            print(f'dur:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'SET'):\n","        if (re.match('twice', org)):\n","            nor = 'R2'\n","    return nor"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:05:03.778739Z","iopub.status.busy":"2023-12-02T12:05:03.778266Z","iopub.status.idle":"2023-12-02T12:05:03.800555Z","shell.execute_reply":"2023-12-02T12:05:03.799417Z","shell.execute_reply.started":"2023-12-02T12:05:03.778702Z"},"trusted":true},"outputs":[],"source":["def Spilt2Words(name, f, fa, date1, date0, time1, time0, duration1, duration0, set1, set0):\n","    tok = []\n","    ner = []\n","    lidx = 0\n","    ridx = 0\n","    while True:\n","        # remove last '\\n'\n","        ans_info = fa.readline()[:-1].split('\\t')\n","        # remove normalized DATE/TIME\n","        if (ans_info[1] == 'DATE'):\n","            if (re.match(DATEs, ans_info[4])):\n","                date1 += 1\n","                #print(f'match DATE {ans_info[4:]}')\n","                nor = Normalize('DATE', ans_info[4], ans_info[5])\n","            else:\n","                date0 += 1\n","                #print(f'miss  DATE {ans_info}')\n","            ans_info = ans_info[:-1]\n","        elif (ans_info[1] == 'TIME'):\n","            if (re.match(TIMEs, ans_info[4])):\n","                time1 += 1\n","                #print(f'match TIME {ans_info[4:]}')\n","                nor = Normalize('TIME', ans_info[4], ans_info[5])\n","            else:\n","                time0 += 1\n","                #print(f'miss  TIME {ans_info[4:]}')\n","            ans_info = ans_info[:-1]\n","        elif (ans_info[1] == 'DURATION'):\n","            if (re.match(DURATIONs, ans_info[4])):\n","                duration1 += 1\n","                #print(f'match DURATION {ans_info[4:]}')\n","                nor = Normalize('DURATION', ans_info[4], ans_info[5])\n","            else:\n","                duration0 += 1\n","                #print(f'miss  DURATION {ans_info[4:]}')\n","            ans_info = ans_info[:-1]\n","        elif (ans_info[1] == 'SET'):\n","            if (re.match(SETs, ans_info[4])):\n","                set1 += 1\n","                #print(f'match SET {ans_info[4:]}')\n","                nor = Normalize('SET', ans_info[4], ans_info[5])\n","            else:\n","                set0 += 1\n","                #print(f'miss  SET {ans_info[4:]}')\n","            ans_info = ans_info[:-1]\n","            \n","        if (ans_info[1] != 'OTHER'): entity_count[org_label2id[ans_info[1]]] += 1\n","            \n","        ent_lidx, ent_ridx = int(ans_info[2]), int(ans_info[3])\n","\n","        # find next ans_info\n","        while True:\n","            word = ''\n","            # find next word lidx\n","            while True:\n","                nxt_char = f.read(1)\n","                if (nxt_char == ' ' or nxt_char == '\\n' or nxt_char == '\\t'): \n","                    lidx += 1\n","                else: \n","                    word += nxt_char\n","                    break\n","            ridx = lidx\n","            # find next word ridx\n","            while True:\n","                char_pos = f.tell()\n","                nxt_char = f.read(1)\n","                if (nxt_char == ' ' or nxt_char == '\\n' or nxt_char == '\\t' or ridx + 1 == ent_ridx):\n","                    ridx += 1\n","                    f.seek(char_pos)\n","                    break\n","                else:\n","                    ridx += 1\n","                    word += nxt_char\n","                \n","            line_end = 0\n","            # remove '\\n' in last word\n","            if (word[:-1] == '\\n'): \n","                line_end = 1\n","                word = word[:-1]\n","            # truncate beginning of the word if it is an entity word\n","            while (lidx < ent_lidx and ridx > ent_lidx and ridx <= ent_ridx):\n","                lidx += 1\n","                word = word[1:]\n","                \n","            tok.append(word)\n","            \n","            if (lidx < ent_lidx):\n","                ner.append(label2id['OTHER'])\n","            elif (lidx == ent_lidx):\n","                ner.append(label2id['B-' + ans_info[1]])\n","            elif (ridx <= ent_ridx):\n","                ner.append(label2id['I-' + ans_info[1]])\n","            \n","            lidx = ridx\n","            \n","            if (ridx == ent_ridx): # found the last word of entity, move to next answer info\n","                break\n","        \n","        info_pos = fa.tell()\n","        nxt_info = fa.readline()[:-1].split('\\t')\n","        fa.seek(info_pos)\n","        # nxt_info is in next file\n","        if (nxt_info[0] != name): \n","            break\n","        # nxt_info is in current file but has overlap in current info\n","        if (int(nxt_info[3]) <= ent_ridx):\n","            nxt_info = fa.readline()\n","            \n","    return tok, ner, date1, date0, time1, time0, duration1, duration0, set1, set0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:30:57.182214Z","iopub.status.busy":"2023-12-02T12:30:57.181489Z","iopub.status.idle":"2023-12-02T12:30:57.192105Z","shell.execute_reply":"2023-12-02T12:30:57.191169Z","shell.execute_reply.started":"2023-12-02T12:30:57.182181Z"},"trusted":true},"outputs":[],"source":["def Segmentation(ds_id, ds_tok, ds_ner, id, tok, ner, l):\n","    while (len(ner) >= l):\n","        ridx = l\n","        k = random.randint(0, 1)\n","        while (ridx > 0 and ridx < len(ner) and id2label[ner[ridx]] != 'OTHER'):\n","            if (k): \n","                ridx += 1\n","            else:\n","                ridx -= 1\n","        if (ridx == 0):\n","            ridx = len(ner)\n","        elif (ridx < len(ner)):\n","            ridx += 1\n","        find = 0\n","        for i in range(len(ner[:ridx])):\n","            if (ner[:ridx][i] != 0):\n","                find = 1\n","                break\n","        if (find == 1):\n","            ds_id.append(id)\n","            ds_tok.append(tok[:ridx])\n","            ds_ner.append(ner[:ridx])\n","        tok = tok[ridx:]\n","        ner = ner[ridx:]\n","    if (len(ner) > 0):\n","        find = 0\n","        for i in range(len(ner)):\n","            if (ner[i] != 0):\n","                find = 1\n","                break\n","        if (find == 1):\n","            ds_id.append(id)\n","            ds_tok.append(tok)\n","            ds_ner.append(ner)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Need to change path name with different directory structure\n","\n","ds_dict = {'id':[], 'tokens':[], 'ner_tags':[]}\n","eval_dict = {'id':[], 'tokens':[], 'ner_tags':[]}\n","\n","directories = [\n","    '/kaggle/input/nerdataset-phase1/First_Phase_ReleaseCorrection/First_Phase_Release(Correction)/First_Phase_Text_Dataset',\n","    '/kaggle/input/nerdataset-phase1/Second_Phase_Dataset/Second_Phase_Dataset/Second_Phase_Text_Dataset',\n","    '/kaggle/input/nerdataset-phase1/First_Phase_ReleaseCorrection/First_Phase_Release(Correction)/Validation_Release'\n","]\n","answers  = [\n","    '/kaggle/input/nerdataset-phase1/First_Phase_ReleaseCorrection/First_Phase_Release(Correction)/answer.txt',\n","    '/kaggle/input/nerdataset-phase1/Second_Phase_Dataset/Second_Phase_Dataset/answer.txt',\n","    '/kaggle/input/nerdataset-phase1/Validation_Dataset_Answer/answer.txt'\n","]\n","\n","max_word_length = 80\n","\n","date1 ,date0, time1, time0 , duration1, duration0, set1, set0 = 0, 0, 0, 0, 0, 0, 0, 0\n","for i in range(3):\n","    fnames = [f for f in os.listdir(directories[i])]\n","    fnames.sort()\n","    fa = open(answers[i], 'r')\n","    for fname in tqdm(fnames):\n","        #print(fname)\n","        f = open(f'{directories[i]}/{fname}', 'r')\n","        tok, ner, date1, date0, time1, time0, duration1, duration0, set1, set0 = Spilt2Words(fname[:-4], f, fa, date1, date0, time1, time0, duration1, duration0, set1, set0)\n","        if (max_word_length > 0):\n","                Segmentation(ds_dict['id'], ds_dict['tokens'], ds_dict['ner_tags'], fname[:-4], tok, ner, max_word_length)\n","        else:\n","            ds_dict['id'].append(fname[:-4])\n","            ds_dict['tokens'].append(tok)\n","            ds_dict['ner_tags'].append(ner)\n","        f.close()\n","        \n","print('Time information accuarcy:')\n","print(f'DATE: {date1 / (date1 + date0)}')\n","print(f'TIME: {time1 / (time1 + time0)}')\n","print(f'DURATION: {duration1 / (duration1 + duration0)}')\n","print(f'SET: {set1 / (set1 + set0)}')"]},{"cell_type":"markdown","metadata":{},"source":["## Spilt train & dev data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:16:59.042430Z","iopub.status.busy":"2023-12-02T12:16:59.042074Z","iopub.status.idle":"2023-12-02T12:16:59.051136Z","shell.execute_reply":"2023-12-02T12:16:59.050092Z","shell.execute_reply.started":"2023-12-02T12:16:59.042402Z"},"trusted":true},"outputs":[],"source":["def CountSim(train, valid):\n","    tcnt = [0] * len(entity)\n","    vcnt = [0] * len(entity)\n","    for tdata in train:\n","        for t in tdata:\n","            if (t != 0 and id2label[t][0] != 'I'): tcnt[org_label2id[id2label[t][2:]]] += 1\n","    for vdata in valid:\n","        for v in vdata:\n","            if (v != 0 and id2label[v][0] != 'I'): vcnt[org_label2id[id2label[v][2:]]] += 1\n","    tsum = sum(tcnt)\n","    vsum = sum(vcnt)\n","    dist = 0\n","    for i in range(len(entity)):\n","        if (tsum > 0 and vsum > 0):\n","            tcnt[i] = tcnt[i]/tsum\n","            vcnt[i] = vcnt[i]/vsum\n","            dist += abs(tcnt[i] - vcnt[i]) * abs(tcnt[i] - vcnt[i])\n","    return tcnt, vcnt, dist"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:25:59.882394Z","iopub.status.busy":"2023-12-02T12:25:59.881525Z","iopub.status.idle":"2023-12-02T12:26:13.995356Z","shell.execute_reply":"2023-12-02T12:26:13.994370Z","shell.execute_reply.started":"2023-12-02T12:25:59.882358Z"},"trusted":true},"outputs":[],"source":["best_ds_train_valid = Dataset.from_dict(ds_dict).train_test_split(train_size=0.9)\n","best_tpor = [0] * len(entity)\n","best_vpor = [0] * len(entity)\n","best_dist = 1\n","upper_bound = 1.2e-5\n","try_step = 1000\n","while (best_dist > upper_bound):\n","    for i in tqdm(range(try_step)):\n","        cur_ds_train_valid = Dataset.from_dict(ds_dict).train_test_split(train_size=0.8)\n","        cur_tpor, cur_vpor, cur_dist = CountSim(cur_ds_train_valid['train']['ner_tags'], cur_ds_train_valid['test']['ner_tags'])\n","        if (cur_dist < best_dist):\n","            best_ds_train_valid = cur_ds_train_valid\n","            best_tpor = cur_tpor\n","            best_vpor = cur_vpor\n","            best_dist = cur_dist\n","            print(f'New smallest dist = {best_dist}')\n","            if (best_dist < upper_bound):\n","                break\n","\n","x = np.arange(len(entity_names))\n","width = 0.4\n","plt.figure(figsize=(12.8, 4.8))\n","plt.bar(x, best_tpor, width, color='green', label='Train')\n","plt.bar(x + width, best_vpor, width, color='blue', label='Dev')\n","plt.xticks(x + width / 2, entity_names, rotation='vertical')\n","plt.ylabel('Porpotion')\n","plt.title('TrainDev distribution')\n","plt.legend()\n","plt.savefig('TrainDev distribution')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:26:44.062725Z","iopub.status.busy":"2023-12-02T12:26:44.061917Z","iopub.status.idle":"2023-12-02T12:26:44.238673Z","shell.execute_reply":"2023-12-02T12:26:44.237735Z","shell.execute_reply.started":"2023-12-02T12:26:44.062690Z"},"trusted":true},"outputs":[],"source":["raw_ds = DatasetDict({'train': Dataset.from_dict(ds_dict),\n","                  'validation': best_ds_train_valid['test']})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T09:06:04.491541Z","iopub.status.busy":"2023-12-02T09:06:04.491265Z","iopub.status.idle":"2023-12-02T09:06:04.506362Z","shell.execute_reply":"2023-12-02T09:06:04.505534Z","shell.execute_reply.started":"2023-12-02T09:06:04.491515Z"},"trusted":true},"outputs":[],"source":["# Count number of each entity in entire dataset\n","#plt.figure(figsize=(12.8, 4.8))\n","#plt.bar(entity_names,\n","#        entity_count, \n","#        width=0.8, \n","#        bottom=None, \n","#        align='center', \n","#        )\n","#plt.title('Entity Count')\n","#plt.xticks(rotation='vertical')\n","#plt.ylabel('Count')\n","#plt.savefig('Entity Count')\n","#plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:06:11.602264Z","iopub.status.busy":"2023-12-02T12:06:11.601897Z","iopub.status.idle":"2023-12-02T12:06:14.786383Z","shell.execute_reply":"2023-12-02T12:06:14.785366Z","shell.execute_reply.started":"2023-12-02T12:06:11.602230Z"},"trusted":true},"outputs":[],"source":["model_name = 'lakshyakh93/deberta_finetuned_pii'\n","model_checkpoint = model_name\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:06:14.788263Z","iopub.status.busy":"2023-12-02T12:06:14.787775Z","iopub.status.idle":"2023-12-02T12:06:14.795532Z","shell.execute_reply":"2023-12-02T12:06:14.794523Z","shell.execute_reply.started":"2023-12-02T12:06:14.788225Z"},"trusted":true},"outputs":[],"source":["def align_labels_with_tokens(labels, word_ids):\n","    new_labels = []\n","    current_word = None\n","    for word_id in word_ids:\n","        if word_id != current_word:\n","            # Start of a new word!\n","            current_word = word_id\n","            label = -100 if word_id is None else labels[word_id]\n","            new_labels.append(label)\n","        elif word_id is None:\n","            # Special token\n","            new_labels.append(-100)\n","        else:\n","            # Same word as previous token\n","            label = labels[word_id]\n","            # If the label is B-XXX we change it to I-XXX\n","            if label % 2 == 1:\n","                label += 1\n","            new_labels.append(label)\n","\n","    return new_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:06:18.901610Z","iopub.status.busy":"2023-12-02T12:06:18.900813Z","iopub.status.idle":"2023-12-02T12:06:18.907435Z","shell.execute_reply":"2023-12-02T12:06:18.906166Z","shell.execute_reply.started":"2023-12-02T12:06:18.901576Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples['tokens'], truncation=True, is_split_into_words=True\n","    )\n","    all_labels = examples['ner_tags']\n","    new_labels = []\n","    for i, labels in enumerate(all_labels):\n","        word_ids = tokenized_inputs.word_ids(i)\n","        new_labels.append(align_labels_with_tokens(labels, word_ids))\n","\n","    tokenized_inputs['labels'] = new_labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:26:49.481948Z","iopub.status.busy":"2023-12-02T12:26:49.481112Z","iopub.status.idle":"2023-12-02T12:26:54.522601Z","shell.execute_reply":"2023-12-02T12:26:54.521582Z","shell.execute_reply.started":"2023-12-02T12:26:49.481912Z"},"trusted":true},"outputs":[],"source":["tokenized_datasets = raw_ds.map(\n","    tokenize_and_align_labels,\n","    batched=True,\n","    remove_columns=raw_ds['train'].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:26:57.761997Z","iopub.status.busy":"2023-12-02T12:26:57.761184Z","iopub.status.idle":"2023-12-02T12:26:57.768077Z","shell.execute_reply":"2023-12-02T12:26:57.767050Z","shell.execute_reply.started":"2023-12-02T12:26:57.761966Z"},"trusted":true},"outputs":[],"source":["tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:32:50.742709Z","iopub.status.busy":"2023-12-02T12:32:50.741814Z","iopub.status.idle":"2023-12-02T12:32:50.747033Z","shell.execute_reply":"2023-12-02T12:32:50.746073Z","shell.execute_reply.started":"2023-12-02T12:32:50.742675Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","#batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n","#batch['labels']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T09:06:17.006296Z","iopub.status.busy":"2023-12-02T09:06:17.005970Z","iopub.status.idle":"2023-12-02T09:06:17.023915Z","shell.execute_reply":"2023-12-02T09:06:17.022954Z","shell.execute_reply.started":"2023-12-02T09:06:17.006265Z"},"trusted":true},"outputs":[],"source":["#for i in range(2):\n","#    print(tokenized_datasets['train'][i]['labels'])"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:32:53.621343Z","iopub.status.busy":"2023-12-02T12:32:53.620947Z","iopub.status.idle":"2023-12-02T12:32:55.243124Z","shell.execute_reply":"2023-12-02T12:32:55.242276Z","shell.execute_reply.started":"2023-12-02T12:32:53.621312Z"},"trusted":true},"outputs":[],"source":["metric = evaluate.load('seqeval')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T09:06:17.857462Z","iopub.status.busy":"2023-12-02T09:06:17.857173Z","iopub.status.idle":"2023-12-02T09:06:17.862413Z","shell.execute_reply":"2023-12-02T09:06:17.860924Z","shell.execute_reply.started":"2023-12-02T09:06:17.857436Z"},"trusted":true},"outputs":[],"source":["#labels = raw_ds['train'][0]['ner_tags']\n","#labels = [label_names[i] for i in labels]\n","#labels\n","#\n","#predictions = labels.copy()\n","#predictions[2] = 'OTHER'\n","#metric.compute(predictions=[predictions], references=[labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:32:56.761328Z","iopub.status.busy":"2023-12-02T12:32:56.760952Z","iopub.status.idle":"2023-12-02T12:32:56.768825Z","shell.execute_reply":"2023-12-02T12:32:56.767598Z","shell.execute_reply.started":"2023-12-02T12:32:56.761298Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        'precision': all_metrics['overall_precision'],\n","        'recall': all_metrics['overall_recall'],\n","        'f1': all_metrics['overall_f1'],\n","        'accuracy': all_metrics['overall_accuracy'],\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Model & Training Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:32:59.181752Z","iopub.status.busy":"2023-12-02T12:32:59.181098Z","iopub.status.idle":"2023-12-02T12:32:59.218545Z","shell.execute_reply":"2023-12-02T12:32:59.217516Z","shell.execute_reply.started":"2023-12-02T12:32:59.181721Z"},"trusted":true},"outputs":[],"source":["# Can add linear/lstm layers\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class BERT_CRF(nn.Module):\n","    def __init__(self, bert, num_labels, hidden_dim):\n","        super(BERT_CRF, self).__init__()\n","        self.bert = bert\n","        self.num_labels = num_labels\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(input_size=self.num_labels, hidden_size=self.hidden_dim, num_layers=1, bidirectional=True)\n","        self.classifier = nn.Sequential(\n","            #nn.Dropout(0.06),\n","            #nn.Linear(1024, 1024),\n","            nn.Linear(self.num_labels, self.num_labels),\n","            nn.Dropout(0.1),\n","        )\n","        self.crf = CRF(self.num_labels, batch_first=True)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask, labels):\n","        outputs=self.bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        sequence_output = outputs[0]\n","        #h0 = torch.zeros(2, sequence_output.shape[1], self.hidden_dim).to(device)\n","        #c0 = torch.zeros(2, sequence_output.shape[1], self.hidden_dim).to(device)\n","        #sequence_output, (hn, cn) = self.lstm(sequence_output, (h0, c0))\n","        logits = self.classifier(sequence_output)\n","        for i in range(labels.shape[0]):\n","            for j in range(labels.shape[1]):\n","                if (labels[i][j] == -100):\n","                    labels[i][j] = 0\n","                    attention_mask[i][j] = True\n","        loss = -self.crf.forward(emissions=logits, tags=labels, mask=attention_mask.bool())\n","        pred = self.crf.decode(emissions=logits, mask=None)\n","        return {'loss':loss, 'pred':pred}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:33:01.601594Z","iopub.status.busy":"2023-12-02T12:33:01.601232Z","iopub.status.idle":"2023-12-02T12:33:47.625456Z","shell.execute_reply":"2023-12-02T12:33:47.624622Z","shell.execute_reply.started":"2023-12-02T12:33:01.601565Z"},"trusted":true},"outputs":[],"source":["model = BERT_CRF(\n","    bert=AutoModelForTokenClassification.from_pretrained(\n","        model_checkpoint,\n","        id2label=id2label,\n","        label2id=label2id,\n","        ignore_mismatched_sizes=True\n","    ), \n","    num_labels=len(label_names), \n","    hidden_dim=128\n",")\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets['train'],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","eval_dataloader = DataLoader(\n","    tokenized_datasets['validation'], collate_fn=data_collator, batch_size=8\n",")\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","accelerator = Accelerator()\n","\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")\n","\n","num_train_epochs = 30\n","num_update_steps_per_epoch = len(train_dataloader)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_scheduler(\n","    'linear',\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Login to push model to hub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:33:47.627251Z","iopub.status.busy":"2023-12-02T12:33:47.626902Z","iopub.status.idle":"2023-12-02T12:33:47.648529Z","shell.execute_reply":"2023-12-02T12:33:47.647592Z","shell.execute_reply.started":"2023-12-02T12:33:47.627224Z"},"trusted":true},"outputs":[],"source":["login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:33:52.922458Z","iopub.status.busy":"2023-12-02T12:33:52.922086Z","iopub.status.idle":"2023-12-02T12:36:11.987798Z","shell.execute_reply":"2023-12-02T12:36:11.986840Z","shell.execute_reply.started":"2023-12-02T12:33:52.922427Z"},"trusted":true},"outputs":[],"source":["model_name = 'deberta-crf-finetuned-phi'\n","#os.mkdir(f'./{model_name}')\n","#repo_name = get_full_repo_name(model_name)\n","#repo = Repository(model_name, clone_from=repo_name)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:36:19.001771Z","iopub.status.busy":"2023-12-02T12:36:19.001406Z","iopub.status.idle":"2023-12-02T12:36:19.008887Z","shell.execute_reply":"2023-12-02T12:36:19.007709Z","shell.execute_reply.started":"2023-12-02T12:36:19.001742Z"},"trusted":true},"outputs":[],"source":["def postprocess(predictions, labels):\n","    predictions = predictions.detach().cpu().clone().numpy()\n","    labels = labels.detach().cpu().clone().numpy()\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    return true_labels, true_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T12:36:23.623375Z","iopub.status.busy":"2023-12-02T12:36:23.622507Z","iopub.status.idle":"2023-12-02T14:16:06.858591Z","shell.execute_reply":"2023-12-02T14:16:06.857243Z","shell.execute_reply.started":"2023-12-02T12:36:23.623342Z"},"trusted":true},"outputs":[],"source":["progress_bar = tqdm(range(num_training_steps))\n","f1_score = []\n","max_f1 = -1\n","for epoch in range(num_train_epochs):\n","    # Training\n","    model.train()\n","    for batch in train_dataloader:\n","        outputs = model(**batch)\n","\n","        loss = outputs['loss']\n","        accelerator.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","    # Evaluation\n","    model.eval()\n","    for batch in eval_dataloader:\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        predictions = outputs['pred']\n","        labels = batch['labels']\n","\n","        # Necessary to pad predictions and labels for being gathered\n","        predictions = accelerator.pad_across_processes(torch.tensor(predictions), dim=1, pad_index=-100)\n","        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n","\n","        predictions_gathered = accelerator.gather(predictions)\n","        labels_gathered = accelerator.gather(labels)\n","\n","        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n","        metric.add_batch(predictions=true_predictions, references=true_labels)\n","\n","    results = metric.compute()\n","    f1_score.append(results['overall_f1'])\n","    print(\n","        f'epoch {epoch}:',\n","        {\n","            key: results[f'overall_{key}']\n","            for key in ['precision', 'recall', 'f1', 'accuracy']\n","        },\n","    )\n","\n","    if (results['overall_f1'] > max_f1):\n","        max_f1 = results['overall_f1']\n","        print(f'new max f1 score = {max_f1}\\n')\n","        #Save and upload\n","        accelerator.wait_for_everyone()\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        torch.save(unwrapped_model, f'{model_name}/pytorch_model{epoch}.bin')\n","        if accelerator.is_main_process:\n","            tokenizer.save_pretrained(model_name)\n","            repo.push_to_hub(\n","                commit_message=f'Training in progress epoch {epoch}', blocking=False\n","            )"]},{"cell_type":"markdown","metadata":{},"source":["## Draw f1 score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-02T09:06:42.926119Z","iopub.status.idle":"2023-12-02T09:06:42.926450Z","shell.execute_reply":"2023-12-02T09:06:42.926299Z","shell.execute_reply.started":"2023-12-02T09:06:42.926284Z"},"trusted":true},"outputs":[],"source":["model_name = model_name.replace('/', '_')\n","plt.plot(f1_score, label = \"f1 score\")\n","# naming the x axis\n","plt.xlabel('epoch')\n","# naming the y axis\n","plt.ylabel('f1 score')\n","# giving a title to my graph\n","title = f'{model_name} max word len = {max_word_length}'\n","plt.title(title)\n","# show a legend on the plot\n","plt.legend()\n","# store fig\n","plt.savefig(model_name)\n","# function to show the plot\n","plt.show()\n","# store score\n","with open(title, \"wb\") as fp:   #Pickling\n","    pickle.dump(f1_score, fp)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["repo_name = get_full_repo_name(model_name)\n","repo = Repository(model_name, clone_from=repo_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-02T09:06:42.934661Z","iopub.status.idle":"2023-12-02T09:06:42.935052Z","shell.execute_reply":"2023-12-02T09:06:42.934888Z","shell.execute_reply.started":"2023-12-02T09:06:42.934867Z"},"trusted":true},"outputs":[],"source":["model = torch.load('/kaggle/working/deberta-crf-finetuned-phi/pytorch_model.bin')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-02T09:06:42.937687Z","iopub.status.idle":"2023-12-02T09:06:42.938043Z","shell.execute_reply":"2023-12-02T09:06:42.937894Z","shell.execute_reply.started":"2023-12-02T09:06:42.937878Z"},"trusted":true},"outputs":[],"source":["p = open('time.txt', 'w')\n","\n","for i in range(2, 3):\n","    fnames = [f for f in os.listdir(\"/kaggle/input/nerdataset-phase1/opendid_test/opendid_test\")]\n","    fnames.sort()\n","    for fname in tqdm(fnames):\n","        base = 0\n","        print(fname)\n","        f = open(f'/kaggle/input/nerdataset-phase1/opendid_test/opendid_test/{fname}', 'r')\n","        NE = []\n","        text = f.readline()\n","        text_len = len(text)\n","        while (text != ''):\n","            #print(len(text))\n","            #print(len(text.split()))\n","            token = tokenizer(text.split(), is_split_into_words=True)\n","            word = token.tokens()\n","            output = model(input_ids=torch.LongTensor([token['input_ids']]).to(device), \n","                           token_type_ids=torch.LongTensor([token['token_type_ids']]).to(device), \n","                           attention_mask=torch.BoolTensor([token['attention_mask']]).to(device), \n","                           labels=torch.LongTensor([token['token_type_ids']]).to(device)\n","                          )\n","            pred = output['pred'][0]\n","            l = len(word)\n","            s = ''\n","            find = 0\n","            for j in range(l):\n","                if (find == 1 and (pred[j] == 0 or id2label[pred[j]][0] == 'B')):\n","                    idx = text.find(s)\n","                    tmp = idx\n","                    text = (idx + len(s)) * '$' + text[idx + len(s):]\n","                    idx += base\n","                    if (idx == -1):\n","                        #print(s)\n","                        print(f'--------{text}--------')\n","                        #print('\\n')\n","                        assert(0)\n","                    if (id2label[pred[j - 1]][2:] == 'TIME' and s[-2:] == 'on'):\n","                        s = s + ' '\n","                        tmp += len(s)\n","                        #print(f'tmp = {text[tmp]}')\n","                        while (len(text) > tmp and re.match('\\d{1}|\\/|\\.', text[tmp])):\n","                            s = s + text[tmp]\n","                            tmp += 1\n","                        NE.append([fname[:-4], id2label[pred[j - 1]][2:], idx, idx + len(s), s])\n","                        print('on')\n","                        #print(s)\n","                    else:\n","                        NE.append([fname[:-4], id2label[pred[j - 1]][2:], idx, idx + len(s), s])\n","                    s = ''\n","                    find = 0\n","                if (pred[j] > 0):\n","                    if (word[j] == 'ï¿½'):\n","                        break\n","                    s += word[j]\n","                    find = 1\n","                    lidx = text.find(s)\n","                    ridx = lidx + len(s)\n","                    #print(f's={s}')\n","                    if (len(text) > ridx and id2label[pred[j + 1]][0] == 'I'):\n","                        if (text.find(s + ' ' + word[j + 1]) != -1):\n","                            s += ' '\n","                            continue\n","                        if (text.find(s + '  ' + word[j + 1]) != -1):\n","                            s += '  '\n","                            continue\n","                        if (text.find(s + '\\n' + word[j + 1]) != -1 and text[ridx] == '\\n'):\n","                            s += '\\n'\n","                            continue\n","                    #print(f's={s}, ridx={ridx}, text[ridx]={text[ridx]}')\n","            \n","            #print(word)\n","            #print(pred)\n","            base += text_len\n","            text = f.readline()\n","            text_len = len(text)\n","        for j in range(len(NE)):\n","            if (NE[j][4][-1] == ' '):\n","                NE[j][3] -= 1\n","                NE[j][4] = NE[j][4][:-1]\n","            if (NE[j][4][-1] == '.'):\n","                NE[j][3] -= 1\n","                NE[j][4] = NE[j][4][:-1]\n","            if (len(NE[j][4]) == 0):\n","                continue\n","            #print(NE[j])\n","            if (NE[j][1] == 'DATE' or NE[j][1] == 'TIME' or NE[j][1] == 'DURATION' or NE[j][1] == 'SET'):\n","                nor = Normalize(NE[j][1], NE[j][4], '')\n","                if (NE[j][1] == 'DATE' and NE[j][4][-1] == ')'):\n","                    nor = Normalize(NE[j][1], NE[j][4][:-1], '')\n","                    p.write(f'{NE[j][0]}\\t{NE[j][1]}\\t{NE[j][2]}\\t{NE[j][3]}\\t{NE[j][4][:-1]}\\t{nor}\\n')\n","                else:\n","                    p.write(f'{NE[j][0]}\\t{NE[j][1]}\\t{NE[j][2]}\\t{NE[j][3]}\\t{NE[j][4]}\\t{nor}\\n')\n","                #print(f'{NE[j][0]}\\t{NE[j][1]}\\t{NE[j][2]}\\t{NE[j][3]}\\t{NE[j][4]}\\t{nor}\\n')\n","        f.close()"]},{"cell_type":"markdown","metadata":{},"source":["## Reference\n","- https://huggingface.co/docs/transformers/tasks/token_classification\n","- https://waynestalk.com/python-bar-charts/\n","- https://www.kaggle.com/discussions/general/65351\n","- https://pytorch-crf.readthedocs.io/en/stable/\n","- https://www.shenxiaohai.me/pytorch-tutorial-intermediate-04/\n","- https://regex101.com/"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3820828,"sourceId":7104065,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
